{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jsonlines\n",
    "# import faiss\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from huggingface_hub import login\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install huggingface_hub\n",
    "# # !pip install ipywidgets\n",
    "# login(os.getenv('hf_token'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install -U langchain-community\n",
    "# !pip install -U langchain-ollama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM,AutoModel\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Encoder( & Retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加載 Dragon 模型的 Tokenizer 和 Model\n",
    "# Load model directly\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/dragon-plus-query-encoder\")\n",
    "# context_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatQA論文中使用的多輪檢索器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/dragon-multiturn-query-encoder\")\n",
    "context_encoder = AutoModel.from_pretrained('nvidia/dragon-multiturn-query-encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading corpus(PubMed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #absolute path\n",
    "# corpus_path = r\"..\\corpus\\pubmed24n0002.jsonl\"\n",
    "\n",
    "\n",
    "# # 讀取 JSONL 格式的文件，將每一行轉換為字典，並提取其中的 'content' 字段\n",
    "# def load_corpus(file_path):\n",
    "#     documents = []\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         for line in file:\n",
    "#             data = json.loads(line)\n",
    "#             # 假設使用 'content' 或 'contents' 進行嵌入\n",
    "#             text = data.get('contents')\n",
    "#             if text:\n",
    "#                 documents.append(text)\n",
    "#     return documents\n",
    "\n",
    "# # 加載你的corpus文件\n",
    "# corpus = load_corpus(corpus_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 轉換成向量\n",
    "def embed_query(query: str):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = context_encoder(**inputs)\n",
    "     # 提取隱藏狀態的最後一層（hidden states），形狀為 (batch_size, sequence_length, hidden_size)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    # 平均化隱藏層的輸出，這樣可以得到一個固定長度的向量（shape: [batch_size, hidden_size]）\n",
    "    embedding = last_hidden_state.mean(dim=1).detach().cpu().numpy()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將每篇文檔轉換為嵌入向量\n",
    "# from tqdm import tqdm\n",
    "# corpus_embeddings = [embed_query(doc) for doc in tqdm(corpus, ascii=True, ncols=100, position=0, leave=True)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "檢視"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存到路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# save_path = '../embeddings/nvidia_pubmed24n0002.npy'\n",
    "\n",
    "# # 保存嵌入向量到文件\n",
    "# np.save(save_path, corpus_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定義embedding函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "import os\n",
    "\n",
    "# 自定義 LangChain 的 Embeddings 類，使用你自己的 embed_query 函數\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts):\n",
    "        return [embed_query(text).flatten().tolist() for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return embed_query(text).flatten().tolist()\n",
    "\n",
    "# 初始化自定義嵌入模型\n",
    "embedding_function = CustomEmbeddings()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectorstore目錄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "persist_directory = '../new_vectorstore'\n",
    "os.makedirs(persist_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重新載入chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入已保存的 Chroma 數據庫\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=embedding_function,  # 使用自定義的嵌入模型\n",
    "    persist_directory=persist_directory,    # 指定存儲資料夾\n",
    ")\n",
    "\n",
    "# 現在可以進行查詢、檢索等操作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors into Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假設 corpus 是你已經準備好的文本列表\n",
    "# corpus_embeddings 已經由 embed_query 函數生成\n",
    "\n",
    "# vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embedding_function)\n",
    "\n",
    "\n",
    "# # 插入文檔和對應的嵌入向量\n",
    "# ids = [f\"doc1_{i}\" for i in range(len(corpus))]\n",
    "# vectorstore.add_texts(texts=corpus, ids=ids)\n",
    "\n",
    "# # 持久化到磁碟\n",
    "# vectorstore.persist()\n",
    "\n",
    "# print(f\"嵌入向量已保存到 {persist_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluate import map_output_to_answer, format_question_with_options\n",
    "# from utils import locate_answer, QADataset\n",
    "# import ollama\n",
    "# from datetime import datetime\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# #HighLight\n",
    "# def evaluate(dataset, model_name=\"llama3.1\", wrong_pred_file=None,output_file=None):\n",
    "#     flag = False\n",
    "#     pred = []\n",
    "\n",
    "#     answer_list = [\"A\", \"B\", \"C\", \"D\"]\n",
    "#     answer2idx = {ans:i+1 for i, ans in enumerate(answer_list)}\n",
    "\n",
    "#     total_len = len(dataset)\n",
    "\n",
    "#     for q_idx in tqdm(range(total_len), desc=\"Processing Questions\"):\n",
    "#         question_only = dataset[q_idx]['question']\n",
    "#         question_option_set = format_question_with_options(dataset[q_idx])\n",
    "\n",
    "#         #相關文檔\n",
    "#         #只納入問題，不納入選項\n",
    "#         revelent_docs = retriever.invoke(question_option_set)\n",
    "#         page_contents = \"\\n\".join(f\"{i+1}. {doc.page_content}\" for i, doc in enumerate(revelent_docs))\n",
    "\n",
    "#         # question = \"\"\"You must output the 'option letter' only from the question below. \n",
    "#         # Example: A\n",
    "#         # # Here is the question: {:s}\n",
    "#         # # Below are the paragraphs for reference:\\n\n",
    "#         # # {:s}\n",
    "#         #  \"\"\".format(question_option_set,page_contents)\n",
    "#         # 使用模型產生答案\n",
    "#         response = ollama.generate(\n",
    "#             model=model_name,\n",
    "#             prompt = \"\"\"\n",
    "#                 System: You must output the 'option letter' only from the question below. \n",
    "#                         Example: A\n",
    "#                         Here are the references:\n",
    "#                         {:s}\n",
    "\n",
    "#                 User: {:s}\n",
    "\n",
    "#                 Assistant:\n",
    "#                 \"\"\".format(page_contents,question_option_set)\n",
    "#         )\n",
    "#         # response = ollama.generate(\n",
    "#         #     model=model_name,\n",
    "#         #     prompt=question,\n",
    "#         # )\n",
    "#         model_output = response['response'].strip().upper()\n",
    "                \n",
    "#         options = dataset[q_idx]['options']\n",
    "#         #if model_output的len<2\n",
    "#         if len(model_output) > 2:\n",
    "#             # 如果model_output包含選項的內容，則直接取出選項，否則做re filter\n",
    "#             pred_answer = next((opt for opt, content in options.items() if model_output.strip().lower() in content.strip().lower()), locate_answer(model_output))\n",
    "#         else:\n",
    "#             pred_answer = locate_answer(model_output)\n",
    "\n",
    "#         # print('[Question]:', question)\n",
    "#         # print('[Model]:', model_output)\n",
    "#         # print('[pred_answer]:', pred_answer)\n",
    "#         # print('truth:', dataset[q_idx]['answer'])\n",
    "#         # Check if the model's output is valid\n",
    "#         if pred_answer and pred_answer in answer_list:\n",
    "#             pred.append(answer_list.index(pred_answer)+1)\n",
    "#         else:\n",
    "#             pred.append(-1)\n",
    "        \n",
    "#         map_output_to_answer(model_output)\n",
    "#         # 所有問答\n",
    "#         if pred_answer == dataset[q_idx]['answer']:\n",
    "#             with open(output_file, 'a', encoding='utf-8') as f:\n",
    "#                 f.write(f\"[Question {q_idx+1}]: {dataset[q_idx]['question']}\\n\")\n",
    "#                 f.write(f\"[Options]: {dataset[q_idx]['options']}\\n\")\n",
    "#                 f.write(f\"[Reference]:\\n {page_contents}\\n\")\n",
    "#                 f.write(f\"[Model Output]: {model_output}\\n\")\n",
    "#                 f.write(f\"[Predicted Answer]: {pred_answer}\\n\")\n",
    "#                 f.write(f\"[True Answer]: {dataset[q_idx]['answer']}\\n\\n\")\n",
    "        \n",
    "#         # 錯誤預測的問題\n",
    "#         if pred_answer != dataset[q_idx]['answer']:\n",
    "#             with open(wrong_pred_file,'a',encoding='utf-8') as f:\n",
    "#                 f.write(f\"[Question {q_idx+1}]: {dataset[q_idx]['question']}\\n\")\n",
    "#                 f.write(f\"[Options]: {dataset[q_idx]['options']}\\n\")\n",
    "#                 f.write(f\"[Reference]:\\n {page_contents}\\n\")\n",
    "#                 f.write(f\"[Model Output]: {model_output}\\n\")\n",
    "#                 f.write(f\"[Predicted Answer]: {pred_answer}\\n\")\n",
    "#                 f.write(f\"[True Answer]: {dataset[q_idx]['answer']}\\n\\n\")\n",
    "\n",
    "#     truth = [answer2idx[item['answer']] for item in dataset]\n",
    "#     if len(pred) < len(truth):\n",
    "#         truth = truth[:len(pred)]\n",
    "#         flag = True\n",
    "\n",
    "#     acc = (np.array(truth) == np.array(pred)).mean()\n",
    "#     std = np.sqrt(acc * (1-acc) / len(truth))\n",
    "#     return acc, std, flag\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 測試Medqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dir = r\"C:\\Users\\ailab\\Desktop\\研究所\\論文\\baseline\\records\"\n",
    "# dataset_name = 'medqa'\n",
    "# dataset = QADataset(dataset_name)\n",
    "# # dataset_name = ['medmcqa','medqa']\n",
    "# # datasets = {key:QADataset(key) for key in dataset_name}\n",
    "# scores = []\n",
    "# current_time = datetime.now().strftime(\"%m-%d\")\n",
    "\n",
    "# save_dir = os.path.join(results_dir, \"baseline\")\n",
    "\n",
    "# for dataset_name in dataset_name:\n",
    "#     print(\"[{:s}] \".format(dataset_name), end=\"\")\n",
    "#     save_dir = os.path.join(results_dir, \"baseline\")\n",
    "#     if os.path.exists(save_dir):\n",
    "#         # 設置目錄和文件名\n",
    "#         daily_dir = os.path.join(save_dir, 'rag', current_time)\n",
    "#         # 檢查已有的編號資料夾，並自動分配新編號\n",
    "#         idx = 1\n",
    "#         while os.path.exists(f\"{daily_dir}-{idx}\"):\n",
    "#             idx += 1\n",
    "#         daily_dir = f\"{daily_dir}-{idx}\"\n",
    "#         output_file = os.path.join(daily_dir, \"full_record\", f\"{current_time}-{idx}.txt\")\n",
    "#         wrong_pred_file = os.path.join(daily_dir, \"wrong_pred\", f\"{current_time}-{ idx}.txt\")\n",
    "#         # useless_ans_dir = os.path.join(daily_dir, \"useless_ans\", f\"{current_time}.txt\")\n",
    "\n",
    "#         # 創建所需的所有目錄，exist_ok=True 允許目錄已存在\n",
    "#         os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "#         os.makedirs(os.path.dirname(wrong_pred_file), exist_ok=True)\n",
    "\n",
    "#         acc, std, flag = evaluate(datasets[dataset_name], wrong_pred_file=wrong_pred_file, output_file=output_file)\n",
    "#         scores.append(acc)\n",
    "#         print(\"mean acc: {:.4f}; proportion std: {:.4f}\".format(acc, std), end=\"\")\n",
    "#         if flag:\n",
    "#             print(\" (NOT COMPLETED)\")\n",
    "#         else:\n",
    "#             print(\"\")\n",
    "#         with open(output_file, 'a', encoding='utf-8') as f:\n",
    "#             f.write(f\"{dataset_name} - mean acc: {acc:.4f}, std: {std:.4f}\\n\")\n",
    "#             if flag:\n",
    "#                 f.write(\" (NOT COMPLETED)\\n\")\n",
    "#             else:\n",
    "#                 f.write(\"\\n\")\n",
    "#     else:\n",
    "#         print(\"NOT STARTED.\")\n",
    "#     if len(scores) > 0:\n",
    "#         print(\"[Average] mean acc: {:.4f}\".format(sum(scores) / len(scores)))\n",
    "        \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
